[
  {
    "type": "message",
    "user": "U02UT1XUGFL",
    "text": "안녕하세요, 8월 탕비실 청소를 LAAL이 맡게 되어 공지드립니다.\n\n8월 3일(목) 늦은 오후에 냉장고 및 탕비실 청소를 진행하겠습니다. 지난달과 마찬가지로 아래 링크의 가이드라인을 참고하셔서 사전에 정리 바랍니다.",
    "ts": "1690887985.737179",
    "thread_ts": "1690887985.737179",
    "attachments": [
      {
        "color": "D0D0D0",
        "fallback": "[July 26th, 2022 2:52 PM] kimjoohyoen: 안녕하세요 학생회 총무 2기 김주현입니다.\n이제 3.5기 학생들도 입학을 앞두고 있어 지속적으로 문제가 제기되어오던 냉장고 및 탕비실 수납공간에 대한 정리를 오는 *수요일(27일) 오후 4시 30분*에 하겠습니다.\n\n이미 두 번의 청소를 통해 공지드렸던 사항이지만 다시 한번 구체적인 냉장고 사용 및 정리 가이드를 알려드리겠습니다.\n\n1. 냉장고에 식품을 보관할 시 반드시 *이름과 보존기간*을 명시해주세요. *이름이나 보존기간 둘 중 하나라도 미비되어 있는 경우 모두 버리도록 하겠습니다.*\n2. 냉장고에 과도하게 공간을 차지하는(특히, 냉동실에 보관하는 대량의 닭가슴살, 밀키트)  식품의 경우 타인의 사용을 제한할 수 있기에 지금까지는 경고 수준에서 그쳤지만, *내일 정리를 포함한 향후 모든 정리 일정에서 이름과 보존기간이 명시되어있다하더라도, 과도하게 자리를 차지하고 있다고 판단할 시 해당 물품의 소유주에게 돌려주거나 이름이 없을 경우 전량 폐기처리하겠습니다.*\n3. 내일 정리에서는 싱크대 서랍장에 보관되어있는 식품, 물품 모두 이름과 보존기간이 명시되어있지 않은 경우 버리겠습니다. *서랍장에 칫솔 등을 포함한 세면도구, 농구공 등도 있는데 식품, 물품 관계없이 모두 폐기처리하겠습니다.* 배달음식을 먹을 때 사용할만한 일회용품 등의 경우는 저희가 따로 보관장소를 추후 말씀드리거나 구역을 제한하겠습니다.\n학교에 등교하시지 않는 분들이라면 정리 기한을 말씀드렸기에 급하게 보관해야하는 물건이 있다면, 학교에 등교한 다른 분들이나 저에게 이름과 보존기간을 명기해달라고 부탁드리는 경우, 이번에는 기입해드리도록 하겠습니다. 다만, 앞으로 각 연구실 별로 청소를 담당하게 될텐데 그 경우에는 더 이상 공지하지 않고 위의 사항을 그대로 실천하겠습니다.\n저희 대학원 학생 수가 더 늘어나지만 저장 공간에 한계가 있는 만큼 원우 여러분들의 적극적인 협조 부탁드립니다.\n\n감사합니다.",
        "author_id": "U01QGKCT067",
        "author_name": "2기_김주현",
        "author_subname": "2기_김주현",
        "author_link": "https://snugsds.slack.com/team/U01QGKCT067",
        "author_icon": "https://secure.gravatar.com/avatar/c6966545056a08d1f4311a48cc662172.jpg?s=48\u0026d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0022-48.png",
        "text": "안녕하세요 학생회 총무 2기 김주현입니다.\n이제 3.5기 학생들도 입학을 앞두고 있어 지속적으로 문제가 제기되어오던 냉장고 및 탕비실 수납공간에 대한 정리를 오는 *수요일(27일) 오후 4시 30분*에 하겠습니다.\n\n이미 두 번의 청소를 통해 공지드렸던 사항이지만 다시 한번 구체적인 냉장고 사용 및 정리 가이드를 알려드리겠습니다.\n\n1. 냉장고에 식품을 보관할 시 반드시 *이름과 보존기간*을 명시해주세요. *이름이나 보존기간 둘 중 하나라도 미비되어 있는 경우 모두 버리도록 하겠습니다.*\n2. 냉장고에 과도하게 공간을 차지하는(특히, 냉동실에 보관하는 대량의 닭가슴살, 밀키트)  식품의 경우 타인의 사용을 제한할 수 있기에 지금까지는 경고 수준에서 그쳤지만, *내일 정리를 포함한 향후 모든 정리 일정에서 이름과 보존기간이 명시되어있다하더라도, 과도하게 자리를 차지하고 있다고 판단할 시 해당 물품의 소유주에게 돌려주거나 이름이 없을 경우 전량 폐기처리하겠습니다.*\n3. 내일 정리에서는 싱크대 서랍장에 보관되어있는 식품, 물품 모두 이름과 보존기간이 명시되어있지 않은 경우 버리겠습니다. *서랍장에 칫솔 등을 포함한 세면도구, 농구공 등도 있는데 식품, 물품 관계없이 모두 폐기처리하겠습니다.* 배달음식을 먹을 때 사용할만한 일회용품 등의 경우는 저희가 따로 보관장소를 추후 말씀드리거나 구역을 제한하겠습니다.\n학교에 등교하시지 않는 분들이라면 정리 기한을 말씀드렸기에 급하게 보관해야하는 물건이 있다면, 학교에 등교한 다른 분들이나 저에게 이름과 보존기간을 명기해달라고 부탁드리는 경우, 이번에는 기입해드리도록 하겠습니다. 다만, 앞으로 각 연구실 별로 청소를 담당하게 될텐데 그 경우에는 더 이상 공지하지 않고 위의 사항을 그대로 실천하겠습니다.\n저희 대학원 학생 수가 더 늘어나지만 저장 공간에 한계가 있는 만큼 원우 여러분들의 적극적인 협조 부탁드립니다.\n\n감사합니다.",
        "from_url": "https://snugsds.slack.com/archives/C01Q37T9JLW/p1658814777571799",
        "mrkdwn_in": [
          "text"
        ],
        "blocks": null,
        "footer": "Slack Conversation",
        "ts": 1658814777.571799
      }
    ],
    "reply_count": 2,
    "replies": [
      {
        "user": "U0302PLQ34Y",
        "ts": "1691050318.928639"
      },
      {
        "user": "U0302PLQ34Y",
        "ts": "1691050594.781729"
      }
    ],
    "latest_reply": "1691050594.781729",
    "team": "T01QA5S537V",
    "reactions": [
      {
        "name": "+1",
        "count": 2,
        "users": [
          "U02UCD2C9V5",
          "U0406PM050F"
        ]
      },
      {
        "name": "+1::skin-tone-2",
        "count": 1,
        "users": [
          "U02V5LTRU2V"
        ]
      }
    ],
    "replace_original": false,
    "delete_original": false,
    "metadata": {
      "event_type": "",
      "event_payload": null
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "qBb",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "안녕하세요, 8월 탕비실 청소를 LAAL이 맡게 되어 공지드립니다.\n\n8월 3일(목) 늦은 오후에 냉장고 및 탕비실 청소를 진행하겠습니다. 지난달과 마찬가지로 아래 링크의 가이드라인을 참고하셔서 사전에 정리 바랍니다."
              }
            ]
          }
        ]
      }
    ],
    "user_team": "T01QA5S537V",
    "source_team": "T01QA5S537V",
    "user_profile": {
      "avatar_hash": "",
      "image_72": "https://secure.gravatar.com/avatar/a2dd1abfbec20efd22e870a1fb92a3b8.jpg?s=72\u0026d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0013-72.png",
      "first_name": "3기_김소정_94",
      "real_name": "3기_김소정_94",
      "display_name": "3기_김소정_94",
      "team": "T01QA5S537V",
      "name": "melissasj",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "reply_users_count": 1,
    "reply_users": [
      "U0302PLQ34Y"
    ]
  },
  {
    "client_msg_id": "5a0b1a52-7412-4058-a651-238eb022da63",
    "type": "message",
    "user": "U02V5LS84JV",
    "text": "안녕하세요, 이번주 금요일과 다음주 수요일에 있을 BK 세미나 공지드립니다.\n\n*2023. 8. 4.(금) 11:00 - 12:00* \n*Speaker:* Jason D. Lee, Princeton Unversity\n*Title*: Feature Learning in Deep Learning\n\n*Abstract:*\nWe focus on the task of learning a single index model \\sigma(w* x) with respect to the isotropic Gaussian distribution in d dimensions, including the special case when \\sigma is a kth order hermite which corresponds to the Gaussian analog of parity learning. Prior work has shown that the sample complexity of learning w* is governed by the information exponent k* of the link function \\sigma, which is defined as the index of the first nonzero Hermite coefficient of \\sigma. Prior upper bounds have shown that n \u0026gt; d^{k*-1} samples suffice for learning w* and that this is tight for online SGD (Ben Arous et al., 2020). However, the CSQ lower bound for gradient based methods only shows that n \u0026gt; d^{k*/2} samples are necessary. In this work, we close the gap between the upper and lower bounds by showing that online SGD on a smoothed loss learns w* with n \u0026gt; d^{k*/2} samples.\n\nNext, we turn to the problem of learning multi index models f(x) = g(Ux), where U encodes a latent representation of low dimension. Significant prior work has established that neural networks trained by gradient descent behave like kernel methods, despite significantly worse empirical performance of kernel methods. However, in this work we demonstrate that for this large class of functions that there is a large gap between kernel methods and gradient descent on a two-layer neural network, by showing that gradient descent learns representations relevant to the target task. We also demonstrate that these representations allow for efficient transfer learning, which is impossible in the kernel regime. Specifically, we consider the problem of learning polynomials which depend on only a few relevant directions, i.e. of the form f*(x)=g(Ux) where U is d by r. When the degree of f* is p, it is known that n≍dp samples are necessary to learn f* in the kernel regime. Our primary result is that gradient descent learns a representation of the data which depends only on the directions relevant to f*. This results in an improved sample complexity of n≍d^2r+drp. Furthermore, in a transfer learning setup where the data distributions in the source and target domain share the same representation U but have different polynomial heads we show that a popular heuristic for transfer learning has a target sample complexity independent of d.\n\n*Bio*:\nJason Lee is an associate professor in Electrical Engineering and Computer Science (secondary) at Princeton University. Prior to that, he was in the Data Science and Operations department at the University of Southern California and a postdoctoral researcher at UC Berkeley working with Michael I. Jordan. Jason received his PhD at Stanford University advised by Trevor Hastie and Jonathan Taylor. His research interests are in the theory of machine learning, optimization, and statistics. Lately, he has worked on the foundations of deep learning, representation learning, and reinforcement learning. He has received an NSF Career Award, ONR Young Investigator Award in Mathematical Data Science, Sloan Research Fellowship, NeurIPS Best Student Paper Award and Finalist for the Best Paper Prize for Young Researchers in Continuous Optimization.",
    "ts": "1690889958.950239",
    "files": [
      {
        "id": "F05KQG7CYHG",
        "created": 1690889943,
        "timestamp": 1690889943,
        "name": "unnamed1.jpeg",
        "title": "unnamed1.jpeg",
        "mimetype": "image/jpeg",
        "image_exif_rotation": 0,
        "filetype": "jpg",
        "pretty_type": "JPEG",
        "user": "U02V5LS84JV",
        "mode": "hosted",
        "editable": false,
        "is_external": false,
        "external_type": "",
        "size": 693256,
        "url": "",
        "url_download": "",
        "url_private": "attachments/F05KQG7CYHG-unnamed1.jpeg",
        "url_private_download": "attachments/F05KQG7CYHG-unnamed1.jpeg",
        "original_h": 3840,
        "original_w": 2880,
        "thumb_64": "https://files.slack.com/files-tmb/T01QA5S537V-F05KQG7CYHG-73d988add9/unnamed1_64.jpg",
        "thumb_80": "https://files.slack.com/files-tmb/T01QA5S537V-F05KQG7CYHG-73d988add9/unnamed1_80.jpg",
        "thumb_160": "https://files.slack.com/files-tmb/T01QA5S537V-F05KQG7CYHG-73d988add9/unnamed1_160.jpg",
        "thumb_360": "https://files.slack.com/files-tmb/T01QA5S537V-F05KQG7CYHG-73d988add9/unnamed1_360.jpg",
        "thumb_360_gif": "",
        "thumb_360_w": 270,
        "thumb_360_h": 360,
        "thumb_480": "https://files.slack.com/files-tmb/T01QA5S537V-F05KQG7CYHG-73d988add9/unnamed1_480.jpg",
        "thumb_480_w": 360,
        "thumb_480_h": 480,
        "thumb_720": "https://files.slack.com/files-tmb/T01QA5S537V-F05KQG7CYHG-73d988add9/unnamed1_720.jpg",
        "thumb_720_w": 540,
        "thumb_720_h": 720,
        "thumb_960": "https://files.slack.com/files-tmb/T01QA5S537V-F05KQG7CYHG-73d988add9/unnamed1_960.jpg",
        "thumb_960_w": 720,
        "thumb_960_h": 960,
        "thumb_1024": "https://files.slack.com/files-tmb/T01QA5S537V-F05KQG7CYHG-73d988add9/unnamed1_1024.jpg",
        "thumb_1024_w": 768,
        "thumb_1024_h": 1024,
        "permalink": "https://snugsds.slack.com/files/U02V5LS84JV/F05KQG7CYHG/unnamed1.jpeg",
        "permalink_public": "https://slack-files.com/T01QA5S537V-F05KQG7CYHG-a2be65c2b6",
        "edit_link": "",
        "preview": "",
        "preview_highlight": "",
        "lines": 0,
        "lines_more": 0,
        "is_public": false,
        "public_url_shared": false,
        "channels": null,
        "groups": null,
        "ims": null,
        "initial_comment": {},
        "comments_count": 0,
        "num_stars": 0,
        "is_starred": false,
        "shares": {
          "public": null,
          "private": null
        }
      }
    ],
    "reactions": [
      {
        "name": "+1",
        "count": 4,
        "users": [
          "U01QV1SQJ57",
          "U02ULBH3TEJ",
          "U02UJC5BU8P",
          "U01QNSM488Z"
        ]
      }
    ],
    "replace_original": false,
    "delete_original": false,
    "metadata": {
      "event_type": "",
      "event_payload": null
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "B47n",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "안녕하세요, 이번주 금요일과 다음주 수요일에 있을 BK 세미나 공지드립니다.\n\n"
              },
              {
                "type": "text",
                "text": "2023. 8. 4.(금) 11:00 - 12:00 ",
                "style": {
                  "bold": true
                }
              },
              {
                "type": "text",
                "text": "\n"
              },
              {
                "type": "text",
                "text": "Speaker: ",
                "style": {
                  "bold": true
                }
              },
              {
                "type": "text",
                "text": "Jason D. Lee, Princeton Unversity\n"
              },
              {
                "type": "text",
                "text": "Title",
                "style": {
                  "bold": true
                }
              },
              {
                "type": "text",
                "text": ": Feature Learning in Deep Learning\n\n"
              },
              {
                "type": "text",
                "text": "Abstract:",
                "style": {
                  "bold": true
                }
              },
              {
                "type": "text",
                "text": "\nWe focus on the task of learning a single index model \\sigma(w* x) with respect to the isotropic Gaussian distribution in d dimensions, including the special case when \\sigma is a kth order hermite which corresponds to the Gaussian analog of parity learning. Prior work has shown that the sample complexity of learning w* is governed by the information exponent k* of the link function \\sigma, which is defined as the index of the first nonzero Hermite coefficient of \\sigma. Prior upper bounds have shown that n \u003e d^{k*-1} samples suffice for learning w* and that this is tight for online SGD (Ben Arous et al., 2020). However, the CSQ lower bound for gradient based methods only shows that n \u003e d^{k*/2} samples are necessary. In this work, we close the gap between the upper and lower bounds by showing that online SGD on a smoothed loss learns w* with n \u003e d^{k*/2} samples.\n\nNext, we turn to the problem of learning multi index models f(x) = g(Ux), where U encodes a latent representation of low dimension. Significant prior work has established that neural networks trained by gradient descent behave like kernel methods, despite significantly worse empirical performance of kernel methods. However, in this work we demonstrate that for this large class of functions that there is a large gap between kernel methods and gradient descent on a two-layer neural network, by showing that gradient descent learns representations relevant to the target task. We also demonstrate that these representations allow for efficient transfer learning, which is impossible in the kernel regime. Specifically, we consider the problem of learning polynomials which depend on only a few relevant directions, i.e. of the form f*(x)=g(Ux) where U is d by r. When the degree of f* is p, it is known that n≍dp samples are necessary to learn f* in the kernel regime. Our primary result is that gradient descent learns a representation of the data which depends only on the directions relevant to f*. This results in an improved sample complexity of n≍d^2r+drp. Furthermore, in a transfer learning setup where the data distributions in the source and target domain share the same representation U but have different polynomial heads we show that a popular heuristic for transfer learning has a target sample complexity independent of d.\n\n"
              },
              {
                "type": "text",
                "text": "Bio",
                "style": {
                  "bold": true
                }
              },
              {
                "type": "text",
                "text": ":\nJason Lee is an associate professor in Electrical Engineering and Computer Science (secondary) at Princeton University. Prior to that, he was in the Data Science and Operations department at the University of Southern California and a postdoctoral researcher at UC Berkeley working with Michael I. Jordan. Jason received his PhD at Stanford University advised by Trevor Hastie and Jonathan Taylor. His research interests are in the theory of machine learning, optimization, and statistics. Lately, he has worked on the foundations of deep learning, representation learning, and reinforcement learning. He has received an NSF Career Award, ONR Young Investigator Award in Mathematical Data Science, Sloan Research Fellowship, NeurIPS Best Student Paper Award and Finalist for the Best Paper Prize for Young Researchers in Continuous Optimization."
              }
            ]
          }
        ]
      }
    ],
    "user_team": "",
    "source_team": "",
    "user_profile": {
      "avatar_hash": "",
      "image_72": "https://secure.gravatar.com/avatar/29cb1f72596c97242d862268a3baf61c.jpg?s=72\u0026d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0026-72.png",
      "first_name": "이효진",
      "real_name": "이효진",
      "display_name": "3기_이효진",
      "team": "T01QA5S537V",
      "name": "sarahhyojin",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "reply_users_count": 0,
    "reply_users": null
  },
  {
    "client_msg_id": "8d41d6b3-7621-40ba-9d85-357da2dd437a",
    "type": "message",
    "user": "U02V5LS84JV",
    "text": "*2023. 8. 9.(수) 11:00 - 12:00*\n*Speaker:* 전광성 교수, University of Arizona\n*Title*: Revisiting Simple Regret in Multi-Armed Bandits: Fast Rates for Returning a Good Arm\n\n*Abstract*:\nSimple regret is a natural and parameter-free performance criterion for identifying a good arm in multi-armed bandits yet is less popular than the probability of missing the best arm or an \\epsilon-good arm, perhaps due to lack of easy ways to characterize it. In this paper, we achieve improved simple regret upper bounds for both data-rich (T ≥ n) and data-poor regime (T \u0026lt; n) where n$ is the number of arms and T is the number of samples. At its heart is an improved analysis of the well-known Sequential Halving (SH) algorithm that bounds the probability of returning an arm whose mean reward is not within \\epsilon from the best (i.e., not \\epsilon-good) for _any_ choice of \\epsilon\u0026gt;0, although \\epsilon is not an input to SH. We show that this directly implies an optimal simple regret bound of O\\sqrt{n/T}). Furthermore, our upper bound gets smaller as a function of the number of \\epsilon-good arms. This results in an accelerated rate for the (\\epsilon,\\delta)-PAC criterion, which closes the gap between the upper and lower bounds in prior art. For the more challenging data-poor regime, we propose Bracketing SH (BSH) that enjoys the same improvement even without sampling each arm at least once.\nOur empirical study shows that BSH outperforms existing methods on real-world tasks.\n\n*Bio*:\nKwang-Sung Jun is an assistant professor at the CS department, University of Arizona. His research interest is interactive machine learning with a focus on bandit problems, online learning, and confidence bounds. Before joining UA, he was a postdoc at Boston University with Dr. Francesco Orabona. Before then, he spent 9 years at University of Wisconsin-Madison for a PhD degree with Dr. Xiaojin (Jerry) Zhu and a postdoc with Drs. Robert Nowak, Rebecca Willett, and Stephen Wright.",
    "ts": "1690889988.792159",
    "files": [
      {
        "id": "F05KQD1H5FV",
        "created": 1690889980,
        "timestamp": 1690889980,
        "name": "unnamed2.jpeg",
        "title": "unnamed2.jpeg",
        "mimetype": "image/jpeg",
        "image_exif_rotation": 0,
        "filetype": "jpg",
        "pretty_type": "JPEG",
        "user": "U02V5LS84JV",
        "mode": "hosted",
        "editable": false,
        "is_external": false,
        "external_type": "",
        "size": 648874,
        "url": "",
        "url_download": "",
        "url_private": "attachments/F05KQD1H5FV-unnamed2.jpeg",
        "url_private_download": "attachments/F05KQD1H5FV-unnamed2.jpeg",
        "original_h": 3840,
        "original_w": 2880,
        "thumb_64": "https://files.slack.com/files-tmb/T01QA5S537V-F05KQD1H5FV-9d8740f964/unnamed2_64.jpg",
        "thumb_80": "https://files.slack.com/files-tmb/T01QA5S537V-F05KQD1H5FV-9d8740f964/unnamed2_80.jpg",
        "thumb_160": "https://files.slack.com/files-tmb/T01QA5S537V-F05KQD1H5FV-9d8740f964/unnamed2_160.jpg",
        "thumb_360": "https://files.slack.com/files-tmb/T01QA5S537V-F05KQD1H5FV-9d8740f964/unnamed2_360.jpg",
        "thumb_360_gif": "",
        "thumb_360_w": 270,
        "thumb_360_h": 360,
        "thumb_480": "https://files.slack.com/files-tmb/T01QA5S537V-F05KQD1H5FV-9d8740f964/unnamed2_480.jpg",
        "thumb_480_w": 360,
        "thumb_480_h": 480,
        "thumb_720": "https://files.slack.com/files-tmb/T01QA5S537V-F05KQD1H5FV-9d8740f964/unnamed2_720.jpg",
        "thumb_720_w": 540,
        "thumb_720_h": 720,
        "thumb_960": "https://files.slack.com/files-tmb/T01QA5S537V-F05KQD1H5FV-9d8740f964/unnamed2_960.jpg",
        "thumb_960_w": 720,
        "thumb_960_h": 960,
        "thumb_1024": "https://files.slack.com/files-tmb/T01QA5S537V-F05KQD1H5FV-9d8740f964/unnamed2_1024.jpg",
        "thumb_1024_w": 768,
        "thumb_1024_h": 1024,
        "permalink": "https://snugsds.slack.com/files/U02V5LS84JV/F05KQD1H5FV/unnamed2.jpeg",
        "permalink_public": "https://slack-files.com/T01QA5S537V-F05KQD1H5FV-dbad41060d",
        "edit_link": "",
        "preview": "",
        "preview_highlight": "",
        "lines": 0,
        "lines_more": 0,
        "is_public": false,
        "public_url_shared": false,
        "channels": null,
        "groups": null,
        "ims": null,
        "initial_comment": {},
        "comments_count": 0,
        "num_stars": 0,
        "is_starred": false,
        "shares": {
          "public": null,
          "private": null
        }
      }
    ],
    "reactions": [
      {
        "name": "+1",
        "count": 4,
        "users": [
          "U01QV1SQJ57",
          "U02ULBH3TEJ",
          "U02UJC5BU8P",
          "U01QNSM488Z"
        ]
      }
    ],
    "replace_original": false,
    "delete_original": false,
    "metadata": {
      "event_type": "",
      "event_payload": null
    },
    "blocks": [
      {
        "type": "rich_text",
        "block_id": "xTm",
        "elements": [
          {
            "type": "rich_text_section",
            "elements": [
              {
                "type": "text",
                "text": "2023. 8. 9.(수) 11:00 - 12:00",
                "style": {
                  "bold": true
                }
              },
              {
                "type": "text",
                "text": "\n"
              },
              {
                "type": "text",
                "text": "Speaker: ",
                "style": {
                  "bold": true
                }
              },
              {
                "type": "text",
                "text": "전광성 교수, University of Arizona\n"
              },
              {
                "type": "text",
                "text": "Title",
                "style": {
                  "bold": true
                }
              },
              {
                "type": "text",
                "text": ": Revisiting Simple Regret in Multi-Armed Bandits: Fast Rates for Returning a Good Arm\n\n"
              },
              {
                "type": "text",
                "text": "Abstract",
                "style": {
                  "bold": true
                }
              },
              {
                "type": "text",
                "text": ":\nSimple regret is a natural and parameter-free performance criterion for identifying a good arm in multi-armed bandits yet is less popular than the probability of missing the best arm or an \\epsilon-good arm, perhaps due to lack of easy ways to characterize it. In this paper, we achieve improved simple regret upper bounds for both data-rich (T ≥ n) and data-poor regime (T \u003c n) where n$ is the number of arms and T is the number of samples. At its heart is an improved analysis of the well-known Sequential Halving (SH) algorithm that bounds the probability of returning an arm whose mean reward is not within \\epsilon from the best (i.e., not \\epsilon-good) for "
              },
              {
                "type": "text",
                "text": "any",
                "style": {
                  "italic": true
                }
              },
              {
                "type": "text",
                "text": " choice of \\epsilon\u003e0, although \\epsilon is not an input to SH. We show that this directly implies an optimal simple regret bound of O\\sqrt{n/T}). Furthermore, our upper bound gets smaller as a function of the number of \\epsilon-good arms. This results in an accelerated rate for the (\\epsilon,\\delta)-PAC criterion, which closes the gap between the upper and lower bounds in prior art. For the more challenging data-poor regime, we propose Bracketing SH (BSH) that enjoys the same improvement even without sampling each arm at least once.\nOur empirical study shows that BSH outperforms existing methods on real-world tasks.\n\n"
              },
              {
                "type": "text",
                "text": "Bio",
                "style": {
                  "bold": true
                }
              },
              {
                "type": "text",
                "text": ":\nKwang-Sung Jun is an assistant professor at the CS department, University of Arizona. His research interest is interactive machine learning with a focus on bandit problems, online learning, and confidence bounds. Before joining UA, he was a postdoc at Boston University with Dr. Francesco Orabona. Before then, he spent 9 years at University of Wisconsin-Madison for a PhD degree with Dr. Xiaojin (Jerry) Zhu and a postdoc with Drs. Robert Nowak, Rebecca Willett, and Stephen Wright."
              }
            ]
          }
        ]
      }
    ],
    "user_team": "",
    "source_team": "",
    "user_profile": {
      "avatar_hash": "",
      "image_72": "https://secure.gravatar.com/avatar/29cb1f72596c97242d862268a3baf61c.jpg?s=72\u0026d=https%3A%2F%2Fa.slack-edge.com%2Fdf10d%2Fimg%2Favatars%2Fava_0026-72.png",
      "first_name": "이효진",
      "real_name": "이효진",
      "display_name": "3기_이효진",
      "team": "T01QA5S537V",
      "name": "sarahhyojin",
      "is_restricted": false,
      "is_ultra_restricted": false
    },
    "reply_users_count": 0,
    "reply_users": null
  }
]
